{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "#import pyreadr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import pickle\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "from scipy.optimize import lsq_linear\n",
    "import sklearn\n",
    "#from mlutils import dataset, connector\n",
    "pd.set_option('display.max_columns',1000)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./config.properties']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = './config.properties'\n",
    "configParser.read(configFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dept_to_run=configParser.get('modelling', 'dept_to_run')\n",
    "# dept_to_run=[int(each_dept) for each_dept in dept_to_run.split(',')]\n",
    "\n",
    "#input path\n",
    "item_matl_path = (configParser.get('data_paths', 'prj_dir_path') \\\n",
    "                              + configParser.get('data_paths', 'item_matl_model_data_csv_path'))\n",
    "\n",
    "\n",
    "\n",
    "single_estimates_path = (configParser.get('data_paths', 'prj_dir_path') \\\n",
    "                              + configParser.get('estimates', 'single_estimates_path'))\n",
    "single_estimates_coo_path = (configParser.get('data_paths', 'prj_dir_path') \\\n",
    "                              + configParser.get('estimates', 'single_estimates_coo_path'))\n",
    "ihs_estimates_path=configParser.get('estimates', 'ihs_estimates')\n",
    "ihs_estimates_coo_path=configParser.get('estimates', 'ihs_estimates_coo')\n",
    "\n",
    "#output path\n",
    "bootstrap_estimates_path = (configParser.get('data_paths', 'prj_dir_path') \\\n",
    "                              + configParser.get('estimates', 'bootstrap_estimates'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22626, 86)\n",
      "CPU times: user 305 ms, sys: 14.5 ms, total: 319 ms\n",
      "Wall time: 323 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#reading item material table\n",
    "item_matl=pd.read_csv(item_matl_path,low_memory=False)\n",
    "print(item_matl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing calculated Single Item estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(single_estimates_coo_path, 'rb') as handle:\n",
    "    single_item_coo_estimates = pickle.load(handle)\n",
    "with open(single_estimates_path, 'rb') as handle:\n",
    "    single_item_estimates = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing calculated IHS cost estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ihs=pd.read_pickle(ihs_estimates_path)\n",
    "ihs_coo=pd.read_pickle(ihs_estimates_coo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ihs_estimates_ll=ihs.set_index('matl_desc_cln')['auc_lb_ll'].to_dict()\n",
    "ihs_estimates_ul=ihs.set_index('matl_desc_cln')['auc_lb_ul'].to_dict()\n",
    "ihs_coo_estimates_ll=ihs_coo.set_index('matl_desc_cln_coo')['auc_lb_ll'].to_dict()\n",
    "ihs_coo_estimates_ul=ihs_coo.set_index('matl_desc_cln_coo')['auc_lb_ul'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_misc_cost_group_per_dept(item_matl,dept):\n",
    "    return item_matl.loc[item_matl['acctg_dept_nbr']==dept,'misc_cost_group'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running least squares Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModellingDataset(item_matl, dept_no, target, min_matl_freq = 5):\n",
    "    '''\n",
    "    --method performs\n",
    "    1.filtering items only if all materials have greater than 5 occurances across products\n",
    "    2. matl cost is constant by dept and coo combination hence matl_desc and coo is merged\n",
    "    3. misc cost is constant by creating one hot encoding for acctg_dept_nbr_dept_category_nbr_dept_subcatg_nbr_fineline_nbr_vendor_nbr\n",
    "    --inputs:\n",
    "    item_matl: input the item_matl datset\n",
    "    dept_no: input the dept no for which modelling datset needs to be created\n",
    "    \n",
    "    --output:\n",
    "    modelling df where columns is matl_desc_coo combination, index is item_fam_id,values are matl_perc\n",
    "    \n",
    "    '''\n",
    "    dept=item_matl[(item_matl.acctg_dept_nbr==dept_no)&(~item_matl[target].isnull())].copy()\n",
    "    \n",
    "    #filtering items only if  materials have greater than 5 occurances across items in same dept\n",
    "    ans=dept.groupby('matl_desc_cln')['item_nbr'].agg('count')\n",
    "    list_of_items=ans[ans>=min_matl_freq].index\n",
    "    dept['no_of_matl']=dept.groupby('item_nbr')['item_nbr'].transform('count')\n",
    "    dept['flag_freq_matl']=dept['matl_desc_cln'].isin(list_of_items)\n",
    "\n",
    "    dept['sum_flag_freq_matl']=dept.groupby('item_nbr')['flag_freq_matl'].transform('sum')\n",
    "    dept=dept[dept['sum_flag_freq_matl']==dept['no_of_matl']]\n",
    "    #dept=dept.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "#     dept['misc_cost_group']=dept['acctg_dept_nbr'].astype(int).astype('str')+'_'+\\\n",
    "#         dept['dept_category_nbr'].astype(int).astype('str')+'_'+\\\n",
    "#         dept['dept_subcatg_nbr'].astype(int).astype('str')+'_'+\\\n",
    "#         dept['fineline_nbr'].astype(int).astype('str')+'_'+\\\n",
    "#         dept['vendor_nbr'].astype(int).astype('str')\n",
    "    #dept['matl_desc_cln_coo']=dept['matl_desc_cln'].astype('str')+'_'+dept['coo'].astype('str')\n",
    "    \n",
    "    df=pd.DataFrame(index=dept.mds_fam_id.unique(),columns=dept.matl_desc_cln.unique())\n",
    "    \n",
    "    for index,row in dept.iterrows():\n",
    "        df.loc[row['mds_fam_id'],row['matl_desc_cln']]=row['matl_pct']\n",
    "        df.loc[row['mds_fam_id'],target]=row[target]\n",
    "        df.loc[row['mds_fam_id'],'misc_cost_group']=row['misc_cost_group']\n",
    "\n",
    "    df=df.fillna(0)\n",
    "    \n",
    "\n",
    "    #one hot encoding\n",
    "    df_one_hot_encoded=pd.get_dummies(df['misc_cost_group'])\n",
    "    df=df.drop('misc_cost_group',axis=1)\n",
    "\n",
    "    #concat final df\n",
    "    df_concat=pd.concat([df,df_one_hot_encoded],axis=1)\n",
    "    \n",
    "    #dropping duplicates\n",
    "    df_concat=df_concat.drop_duplicates(keep='first')   \n",
    "    \n",
    "    #df=df.loc[df_concat.index,:]\n",
    "    #df_one_hot_encoded=df_one_hot_encoded.loc[df_concat.index,:]\n",
    "\n",
    "    return df_concat #,df,df_one_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowerAndUpperBounds(df,target,\n",
    "                        ihs_coo_estimates_ul,ihs_coo_estimates_ll,\n",
    "                        ihs_estimates_ul,ihs_estimates_ll,\n",
    "                        single_item_coo_estimates,single_item_estimates,misc_cost_groups,\n",
    "                        defaultub=np.inf,defaultlb=0.01):\n",
    "    ub=[]\n",
    "    lb=[]\n",
    "    for each in df.columns:\n",
    "        material=each.split('_')[0]\n",
    "\n",
    "        if each in target:\n",
    "            continue\n",
    "        elif each in ihs_coo_estimates_ul.keys():\n",
    "            #print(each)\n",
    "            lower=ihs_coo_estimates_ll[each]\n",
    "            upper=ihs_coo_estimates_ul[each]\n",
    "        elif material in ihs_estimates_ul.keys():\n",
    "            #print(material)\n",
    "            lower=ihs_estimates_ll[material]\n",
    "            upper=ihs_estimates_ul[material]\n",
    "        elif each in single_item_coo_estimates.keys():\n",
    "            #print(each)\n",
    "            lower=defaultlb\n",
    "            upper=single_item_coo_estimates[each]\n",
    "        elif material in single_item_estimates.keys():\n",
    "            #print(material)\n",
    "            lower=defaultlb\n",
    "            upper=single_item_estimates[material]\n",
    "        elif each in misc_cost_groups:\n",
    "            #print(each)\n",
    "            lower=0.100\n",
    "            upper=0.900\n",
    "        else:\n",
    "            #print(each)\n",
    "            lower=defaultlb\n",
    "            upper=defaultub   \n",
    "\n",
    "        if lower==upper:\n",
    "            lower=upper*0.70\n",
    "\n",
    "        if ~np.isinf(upper):\n",
    "            if (upper-lower)/upper < 0.30:\n",
    "                lower=upper*0.70\n",
    "\n",
    "        ub.append(upper)\n",
    "        lb.append(lower)\n",
    "\n",
    "\n",
    "    ub=np.array(ub)\n",
    "    lb=np.array(lb)\n",
    "    \n",
    "    return lb,ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDependentIndependentVars(df,misc_cost_groups):\n",
    "    #div all df with target col\n",
    "    #df=df.div(df['fc_lb'],axis=0)\n",
    "\n",
    "    #creating independent and dependent variables\n",
    "    X=df.drop('fc_lb',axis=1)\n",
    "\n",
    "    #take backup of columns names in independent variables\n",
    "    cols=X.columns\n",
    "\n",
    "    #creating 2 independent matrices for materials and fine line vendor combination\n",
    "    X1=X.loc[:,~X.columns.isin(misc_cost_groups)]\n",
    "    X2=X.loc[:,X.columns.isin(misc_cost_groups)]\n",
    "    \n",
    "    X1 = X1.div(df['fc_lb'],axis=0)\n",
    "    Y = df['fc_lb']/df['fc_lb']\n",
    "    \n",
    "    \n",
    "    return X1,X2,Y,cols\n",
    "\n",
    "def reshapeArrays(X1,X2,Y,lb,ub,init):\n",
    "    \n",
    "    # m is no of rows , n is no of columns\n",
    "\n",
    "    #converting weight to (n,1) shape\n",
    "    lb=np.array(lb).reshape(lb.shape[0],1)\n",
    "    ub=np.array(ub).reshape(ub.shape[0],1)\n",
    "    init=np.array(init).reshape(init.shape[0],1)\n",
    "\n",
    "    #converting X to (n,m) shape\n",
    "    X1=np.array(X1).T\n",
    "    X2=np.array(X2).T\n",
    "\n",
    "    #matrix multiplication of W.T,X --> (1,n)*(n,m) -->(1,m)\n",
    "\n",
    "    #convering Y to (1,m) shape\n",
    "\n",
    "    Y=np.array(Y).reshape(1,Y.shape[0])\n",
    "    \n",
    "    return X1,X2,Y,lb,ub,init\n",
    "\n",
    "def coeffsInitilization(lb,ub):\n",
    "    #initilization point for the optimizer\n",
    "    init=(lb+ub)/2\n",
    "#     init[np.isinf(init)]=lb[np.isinf(init)]+1e-8\n",
    "    init[np.isinf(init)]=lb[np.isinf(init)] + 1\n",
    "    \n",
    "    return init\n",
    "\n",
    "\n",
    "def rand_coeffsInitilization(lb, ub, coeffsInitPoint = []):\n",
    "    #initilization point for the optimizer\n",
    "    init=(lb+ub)/2\n",
    "    \n",
    "    for k in range(len(lb)):\n",
    "        if np.isinf(init[k]):\n",
    "            init[k]=lb[k] + 1\n",
    "        else:\n",
    "            init[k] = np.random.uniform(low=lb[k], high=ub[k], size = 1)\n",
    "\n",
    "    if len(coeffsInitPoint)==len(lb):\n",
    "        init = (init + coeffsInitPoint)/2\n",
    "    \n",
    "    return init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optmizer\n",
    "\n",
    "def optimize(X1, X2, Y, coeffsInitPoint, lb, ub, epochs=10**6, verbose=True, method='ADAM'\n",
    "             , loss_lambda = 1.0,under_est_pct=0.7):\n",
    "    \n",
    "    #reshape arrays\n",
    "    \n",
    "    X1,X2,Y,lb,ub,coeffsInitPoint=reshapeArrays(X1,X2,Y,lb,ub,coeffsInitPoint)\n",
    "    \n",
    "    item_cnt = Y.shape[1]\n",
    "    \n",
    "    #create tensorflow variables\n",
    "    def constr(a, b):\n",
    "        return lambda x: tf.clip_by_value(x, a, b)\n",
    "\n",
    "\n",
    "    Y = tf.Variable(Y, dtype=tf.float32, name='Y')\n",
    "    coeffs = tf.Variable(coeffsInitPoint, trainable=True, dtype=tf.float32, name='coeffs', constraint=constr(lb,ub))\n",
    "    X1 = tf.Variable(X1, dtype=tf.float32, name='X1')\n",
    "    X2 = tf.Variable(X2, dtype=tf.float32, name='X2')\n",
    "\n",
    "    \n",
    "    #create loss function with non linear equation\n",
    "    \n",
    "    # y=(matl_pct1*matl_cost1+matl_pct2*matl_cost2+...)/(1-fineline_vendor_margin)\n",
    "    def model(X1,X2,coeffs):\n",
    "        raw_mat_cost=tf.matmul(tf.transpose(coeffs[:X1.shape[0]]),X1)\n",
    "        fvp=tf.matmul(tf.transpose(coeffs[X1.shape[0]:]),X2)\n",
    "#         pred=raw_mat_cost/(1-fvp)\n",
    "        pred=raw_mat_cost+fvp\n",
    "        return pred\n",
    "\n",
    "    def loss_fn():\n",
    "        Y_PRED=model(X1,X2, coeffs)\n",
    "\n",
    "#         loss = tf.reduce_mean(tf.math.square(Y_PRED - Y)+ loss_lambda*(Y_PRED - Y)) \n",
    "\n",
    "#         loss = tf.reduce_mean(tf.math.square(Y_PRED - Y) + (item_cnt/1000)*(Y_PRED - Y)) # resulted in 100% underestimation\n",
    "#         loss = tf.reduce_mean(tf.math.square(Y_PRED - Y) + 1*(0.80 - 1*(Y_PRED < Y))) # not differentiable\n",
    "#         loss = tf.reduce_mean(tf.math.square(Y_PRED - Y)) + 1*(0.80 - tf.reduce_mean(Y_PRED < 1)) # not differentiable\n",
    "        \n",
    "        loss = tf.reduce_mean(tf.math.square(Y_PRED - Y)) + loss_lambda*tf.abs(under_est_pct - tf.reduce_mean(tf.where(Y_PRED < Y, 1.0, 0.0)))\n",
    "\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    #optimizer\n",
    "    \n",
    "    if method == 'SGD':\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    " \n",
    "    if method == 'ADAM':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.01,\n",
    "                                       beta_1=0.9,\n",
    "                                       beta_2=0.999,\n",
    "                                       epsilon=1e-07,\n",
    "                                       amsgrad=False)\n",
    "\n",
    "    obj_vals = []\n",
    "    weights = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        #printing loss_function value after every 10000 epochs\n",
    "        if ((verbose) & (epoch % 10000 == 0)):\n",
    "            print(f'step: {epoch}, obj = {loss_fn().numpy():.2f}')\n",
    "            \n",
    "        obj_vals.append(loss_fn().numpy())\n",
    "        weights.append(coeffs.numpy())\n",
    "        \n",
    "        \n",
    "        # early stopping\n",
    "        if epoch>100:\n",
    "            \n",
    "            cur_loss_avg =  np.mean(obj_vals[(epoch-5):(epoch)])\n",
    "            prev_loss_avg = np.mean(obj_vals[(epoch-10):(epoch-6)]) \n",
    "            \n",
    "            if np.abs(cur_loss_avg - prev_loss_avg) < .00001 :\n",
    "                break\n",
    "        \n",
    "        opt.minimize(loss_fn, var_list=coeffs)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'step: {epoch}, obj = {loss_fn().numpy():.2f}')\n",
    "    \n",
    "    return obj_vals, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_resample(n): \n",
    "    return(np.random.randint(low = 0, high = n, size = n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runBootStrapGDModel(df, misc_cost_groups, lb, ub, \n",
    "                        epochs=10**6, verbose=True, method='ADAM', noIterations=0, runBootStrap=False,\n",
    "                        loss_lambda = 1.0):\n",
    "    '''\n",
    "    This methods runs bootstrap least square model with bounds\n",
    "    \n",
    "    input:\n",
    "    df: modelling dataframe\n",
    "    target_col: dependent variable\n",
    "    noIterations: no of iterations to run bootstrap\n",
    "    \n",
    "    output:\n",
    "    estimates dataframe\n",
    "    estimates percentile dataframe\n",
    "    metrics dataframe\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    coeffsInitPoint_avg = coeffsInitilization(lb, ub)\n",
    "    X1,X2,Y,cols=createDependentIndependentVars(df,misc_cost_groups)\n",
    "    \n",
    "    coeffs_list=[]\n",
    "    obj_vals, weights = optimize(X1, X2, Y, \n",
    "                                 coeffsInitPoint_avg, \n",
    "                                 lb, ub, \n",
    "                                 epochs=epochs, \n",
    "                                 verbose=verbose,\n",
    "                                 method=method,\n",
    "                                 loss_lambda = loss_lambda)\n",
    "    \n",
    "    coeffs_list.append(weights[obj_vals.index(min(obj_vals))].reshape(-1))\n",
    "    \n",
    "    coeffsInitPoint_all = coeffs_list[0]\n",
    "    \n",
    "    #bootstrap regression\n",
    "\n",
    "    if runBootStrap==True:\n",
    "        \n",
    "        print('********** bootstrap regression started **********')\n",
    "\n",
    "        X1=X1.reset_index(drop=True)\n",
    "        X2=X2.reset_index(drop=True)\n",
    "\n",
    "        for i in range(0,noIterations):\n",
    "            if i%10==0:\n",
    "                print('***** bootstrap iteration ' + str(i) + ' *****')\n",
    "            \n",
    "            np.random.seed(i)\n",
    "            coeffsInitPoint = rand_coeffsInitilization(lb, ub, coeffsInitPoint_all)\n",
    "#             coeffsInitPoint = coeffsInitPoint_all\n",
    "\n",
    "            indices=simple_resample(X1.shape[0])\n",
    "            X_bootstrap1=X1.loc[indices,:]\n",
    "            X_bootstrap2=X2.loc[indices,:]\n",
    "            y_bootstrap=Y.values[indices]\n",
    "            \n",
    "            obj_vals, weights = optimize(X_bootstrap1,\n",
    "                                         X_bootstrap2,\n",
    "                                         y_bootstrap,\n",
    "                                         coeffsInitPoint,\n",
    "                                         lb,\n",
    "                                         ub,\n",
    "                                         epochs=epochs,\n",
    "                                         verbose=(i%10==0), \n",
    "                                         method=method,\n",
    "                                        loss_lambda = loss_lambda)\n",
    "            \n",
    "            \n",
    "            coeffs_list.append(weights[obj_vals.index(min(obj_vals))].reshape(-1))    \n",
    "\n",
    "        print('********** bootstrap regression done **********')     \n",
    "    \n",
    "    estimates_df=pd.DataFrame(np.array(coeffs_list),columns=cols)\n",
    "    \n",
    "    return estimates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr_nbr = 100\n",
    "dept_lambda_values={\n",
    "    7:[0.0],\n",
    "    11:[0.0],\n",
    "    14:[0.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dept : 7\n",
      "(1789, 267)\n",
      "loss_lambda : 0.0\n",
      "step: 0, obj = 0.12\n",
      "step: 760, obj = 0.01\n",
      "********** bootstrap regression started **********\n",
      "***** bootstrap iteration 0 *****\n",
      "step: 0, obj = 0.04\n",
      "step: 399, obj = 0.01\n",
      "********** bootstrap regression done **********\n",
      "dept : 11\n",
      "(912, 192)\n",
      "loss_lambda : 0.0\n",
      "step: 0, obj = 0.11\n",
      "step: 1248, obj = 0.01\n",
      "********** bootstrap regression started **********\n",
      "***** bootstrap iteration 0 *****\n",
      "step: 0, obj = 0.04\n",
      "step: 503, obj = 0.01\n",
      "********** bootstrap regression done **********\n",
      "dept : 14\n",
      "(1931, 299)\n",
      "loss_lambda : 0.0\n",
      "step: 0, obj = 0.78\n",
      "step: 1526, obj = 0.01\n",
      "********** bootstrap regression started **********\n",
      "***** bootstrap iteration 0 *****\n",
      "step: 0, obj = 0.42\n",
      "step: 1602, obj = 0.01\n",
      "********** bootstrap regression done **********\n",
      "CPU times: user 4min 23s, sys: 1min 16s, total: 5min 40s\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# code for running for all dept\n",
    "for dept in dept_lambda_values.keys():\n",
    "    print('dept :',dept)\n",
    "\n",
    "    #create modelling dataset\n",
    "    df = createModellingDataset(item_matl, dept, 'fc_lb', 1)\n",
    "    \n",
    "    print(df.shape)\n",
    "    pd.Series(df.index).to_csv(bootstrap_estimates_path \n",
    "                               + '/mds_fam_id_train_dept_' + str(dept) + '_iter_' + str(itr_nbr) \\\n",
    "                               + '.csv',index=False)\n",
    "    \n",
    "    #create upper and lower bounds for each materials\n",
    "    misc_cost_groups = return_misc_cost_group_per_dept(item_matl,dept)\n",
    "    \n",
    "    lb,ub = lowerAndUpperBounds(df,'fc_lb',\n",
    "                              ihs_coo_estimates_ul,ihs_coo_estimates_ll,\n",
    "                              ihs_estimates_ul,ihs_estimates_ll,\n",
    "                              single_item_coo_estimates,single_item_estimates,misc_cost_groups,\n",
    "                              np.inf,0.01)\n",
    "\n",
    "\n",
    "\n",
    "    for loss_lambda in dept_lambda_values[dept]:\n",
    "        \n",
    "        print('loss_lambda :',loss_lambda)\n",
    "        \n",
    "        estimates_df = runBootStrapGDModel(df\n",
    "                                     , misc_cost_groups\n",
    "                                     , lb\n",
    "                                     , ub\n",
    "                                     , epochs=10**6\n",
    "                                     , verbose=True\n",
    "                                     , method='ADAM'\n",
    "                                     , noIterations=10\n",
    "                                     , runBootStrap=True\n",
    "                                     , loss_lambda = loss_lambda)\n",
    "    \n",
    "        estimates_df.to_pickle(bootstrap_estimates_path\n",
    "                           +'/estimates_dept_' + str(dept)+ '_lambda_value_' + str(loss_lambda) \n",
    "                            + '_iter_' + str(itr_nbr) +'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
