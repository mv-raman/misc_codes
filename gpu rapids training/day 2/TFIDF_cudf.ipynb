{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 29 17:33:58 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:15:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    39W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cudf import DataFrame\n",
    "import cudf\n",
    "import math\n",
    "from math import log\n",
    "import nvstrings, nvcategory\n",
    "from numba import cuda, float32\n",
    "import time\n",
    "import rmm \n",
    "import numpy as np\n",
    "from cudf.utils import cudautils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 365 ms, sys: 412 ms, total: 777 ms\n",
      "Wall time: 786 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1.6M tweets data\n",
    "text_sents = cudf.read_csv('/data/tweet_data.csv', delimiter=',', names=['note'],skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                note\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1  is upset that he can't update his Facebook by ...\n",
       "2  @Kenichan I dived many times for the ball. Man...\n",
       "3    my whole body feels itchy and like its on fire \n",
       "4  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_special_characters(text_sents):\n",
    "    nvtext_sents = text_sents.note.data\n",
    "    nvtext_sents = nvtext_sents.fillna('-9999')\n",
    "    nvtext_sents = nvtext_sents.replace('[^\\w\\s]','')\n",
    "    nvtext_sents = nvtext_sents.replace('_', '')\n",
    "    nvtext_sents = nvtext_sents.replace('\\s+', ' ')\n",
    "    nvtext_sents = nvtext_sents.lstrip().rstrip()\n",
    "    nvtext_sents_clean = nvtext_sents.lower()\n",
    "    return nvtext_sents_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 107 ms, sys: 376 ms, total: 483 ms\n",
      "Wall time: 480 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nvtext_sents_clean = rm_special_characters(text_sents)\n",
    "docs_length = len(nvtext_sents_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create frequencies of each word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the message id, message length for each word.\n",
    "@cuda.jit\n",
    "def _initDictionary(length_array, id, doc_length, cnt, n, col, row):\n",
    "    tx = cuda.threadIdx.x\n",
    "    bx = cuda.blockIdx.x\n",
    "    bw = cuda.blockDim.x\n",
    "    pos = tx + bx * bw\n",
    "    if pos < n :\n",
    "        idx = pos % col\n",
    "        id[pos] = idx + 1\n",
    "        doc_length[pos] = length_array[idx] + 1\n",
    "        cnt[pos] = 1\n",
    "\n",
    "def initDictionary(length_array, col, row ):\n",
    "    from math import ceil\n",
    "    n = col * row\n",
    "    d_length_array = cuda.to_device(length_array)\n",
    "    id = cuda.device_array(n, dtype=np.int32)\n",
    "    doc_length = cuda.device_array(n, dtype=np.int32)\n",
    "    cnt = cuda.device_array(n, dtype=np.int32)\n",
    "    blocks, threads = ceil(n / 256), 256\n",
    "    _initDictionary[blocks, threads](d_length_array, id, doc_length, cnt, n, col, row)\n",
    "    \n",
    "    return id, doc_length, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_to_series(cat,size):\n",
    "    # keep values of category in GPU device\n",
    "    device_array = rmm.device_array(size, dtype=np.int32)\n",
    "    cat.values(devptr=device_array.device_ctypes_pointer.value)\n",
    "    return cudf.Series(device_array)\n",
    "\n",
    "def create_freq_dict(nvtext_sents_clean):\n",
    "    # split strings by space\n",
    "    nvtext_sents_clean_v1 = nvtext_sents_clean.split(' ')\n",
    "    # get the actual length of each message or document\n",
    "    length_array = nvtext_sents_clean.count(pat = ' ') # actual len = len + 1\n",
    "    # create all parameters of dict in GPU\n",
    "    id, doc_length, cnt = initDictionary(length_array, len(nvtext_sents_clean_v1[0]), len(nvtext_sents_clean_v1))\n",
    "    # category strings\n",
    "    cat = nvcategory.from_strings(*nvtext_sents_clean_v1)\n",
    "    # keep values of category in GPU device\n",
    "    df_v = cat_to_series(cat,len(id))\n",
    "    key_v = cat_to_series(nvcategory.from_strings(cat.keys()),len(cat.keys()))\n",
    "    # Create cudf dataFrame\n",
    "    key = DataFrame({'str': cat.keys(), 'cat': key_v})\n",
    "    df = DataFrame({'cat' : df_v,'cnt' : cnt ,'id' : id,'doc_length':doc_length})\n",
    "   \n",
    "    # create a local dict\n",
    "    #Total number of terms in the document\n",
    "    df1 = df.groupby(['cat','id','doc_length'], method='hash', as_index=False).count()\n",
    "    freqDict_list_local = key.merge(df1, left_on=['cat'],right_on=['cat'])\n",
    "    \n",
    "    #create a global dict\n",
    "    #Number of documents with term t in it\n",
    "    gdf = df1[['cat','cnt']]\n",
    "    gdf['cnt'] = 1\n",
    "    gdf = gdf.groupby(['cat'],method='hash', as_index=False).count()\n",
    "    freqDict_list_global = key.merge(gdf, left_on=['cat'],right_on=['cat'])\n",
    "    freqDict_list_global['global_cnt'] = freqDict_list_global['cnt']\n",
    "    freqDict_list_global.drop_column('cat')\n",
    "    \n",
    "    return freqDict_list_local,freqDict_list_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "freqDict_list_local,freqDict_list_global = create_freq_dict(nvtext_sents_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute TF, IDF and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def logFunction(array,n):\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.blockIdx.x\n",
    "    bw = cuda.blockDim.x\n",
    "    pos = tx + ty * bw\n",
    "    if pos < n:\n",
    "        array[pos] = log(array[pos])\n",
    "\n",
    "def log_function(df, incol, outcol):\n",
    "    from math import ceil\n",
    "    x1 = df[incol].to_gpu_array()\n",
    "    n = x1.size\n",
    "    blocks,threads = ceil(n / 256),256\n",
    "    logFunction[blocks,threads](x1,n)\n",
    "    df[outcol] = x1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF: (number of times that word w occurs in tweet t) ÷ (number of words in t)\n",
    "def computeTF(freqDict_list):\n",
    "    TF_scores = freqDict_list\n",
    "    TF_scores['TF_scores'] = TF_scores['cnt'] / TF_scores['doc_length']\n",
    "    TF_scores.drop_column('doc_length')\n",
    "    TF_scores.drop_column('cnt')\n",
    "    return TF_scores\n",
    "\n",
    "# IDF: log ((total number of tweets) ÷ ( number of tweets where the word w appears ))\n",
    "def computeIDF(TF_scores,freqDict_list_global,docs_length):\n",
    "    IDF_scores = TF_scores.merge(freqDict_list_global,left_on=['str'],right_on=['str'])\n",
    "    IDF_scores['IDF_scores'] = docs_length/IDF_scores['global_cnt']\n",
    "    IDF_scores = log_function(IDF_scores,'IDF_scores','IDF_scores')\n",
    "    return IDF_scores\n",
    "\n",
    "# TF-IDF = TF * IDF\n",
    "def computeTFIDF(TFIDF_scores):\n",
    "    TFIDF_scores['TFIDF_score'] = TFIDF_scores['IDF_scores']*TFIDF_scores['TF_scores']\n",
    "    TFIDF_scores.drop_column('TF_scores')\n",
    "    TFIDF_scores.drop_column('IDF_scores')\n",
    "    TFIDF_scores.drop_column('cat')\n",
    "    TFIDF_scores.drop_column('cnt')\n",
    "    TFIDF_scores.drop_column('global_cnt')\n",
    "    return TFIDF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#4 computeTF()\n",
    "TF_scores = computeTF(freqDict_list_local)\n",
    "#5 computeIDF()\n",
    "IDF_scores = computeIDF(TF_scores,freqDict_list_global,docs_length)\n",
    "#6 computeTFIDF()\n",
    "TFIDF_scores = computeTFIDF(IDF_scores)\n",
    "TFIDF_scores = TFIDF_scores[TFIDF_scores.str.data.isalpha()] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show top 5 words with the highest TFIDF score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "TFIDF_scores = TFIDF_scores.sort_values(by=['TFIDF_score', 'id'] ,ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_scores[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
